'''
Description: compute scattering lookup tables, with the IITM/TM levelA database
FOR EXPERIMENTAL USE
generated by IITM wrapper
Author: Hejun Xie
Date: 2020-09-18 10:16:55
LastEditors: Hejun Xie
LastEditTime: 2020-12-19 11:44:45
'''

# unit test import
import sys
sys.path.append('/home/xhj/wkspcs/Radar-Operator/ZJU_AERO/')

# Global imports
import os
import numpy as np
import xarray as xr
import multiprocessing as mp
import yaml
import itertools
from scipy import stats
from scipy.integrate import quad

# Local imports
from ZJU_AERO.const import global_constants as constants
from ZJU_AERO.hydro import create_hydrometeor

# Whether to regenerate  all lookup tables, even if already present
FORCE_REGENERATION_SCATTER_TABLES = True

with open('db.yaml', 'r') as yamlfile:
    db_cfg = yaml.load(yamlfile, Loader=yaml.SafeLoader)

# Load DBS settings
for key in db_cfg['DBS']:
    locals()[key] = db_cfg['DBS'][key]

# Load EXP settings
epsilon = 1e-6
for key in db_cfg['EXP']:
    locals()[key] = dict()
    for hydrom_type in db_cfg['EXP'][key].keys(): 
        params = db_cfg['EXP'][key][hydrom_type]
        locals()[key][hydrom_type] = np.arange(params[0], params[1]+epsilon, params[2])

# Load DIMENSIONS settings:
for key in db_cfg['DIMENSIONS']:
    if key in ['ELEVATIONS', 'BETAS']:
        params = db_cfg['DIMENSIONS'][key]
        locals()[key] = np.arange(params[0], params[1], params[2])
    else:
        locals()[key] = db_cfg['DIMENSIONS'][key]
    
    if isinstance(locals()[key], list):
        locals()[key] = np.array(locals()[key])


# the key in levelA and levelB dictionaries
real_variables      = [ 'p11_bw', 'p12_bw', 'p13_bw', 'p14_bw',
                        'p21_bw', 'p22_bw', 'p23_bw', 'p24_bw',
                        'p31_bw', 'p32_bw', 'p33_bw', 'p34_bw',
                        'p41_bw', 'p42_bw', 'p43_bw', 'p44_bw']
complex_variables   = [ 's11_fw', 's12_fw', 's21_fw', 's22_fw']


def gaussian_pdf(std=10.0, mean=0.0):
    """Gaussian PDF for orientation averaging.

    Args:
        std: The standard deviation in degrees of the Gaussian PDF
        mean: The mean in degrees of the Gaussian PDF.  This should be a number
          in the interval [0, 180)

    Returns:
        pdf(x), a function that returns the value of the spherical Jacobian- 
        normalized Gaussian PDF with the given STD at x (degrees). It is 
        normalized for the interval [0, 180].
    """
    norm_const = 1.0
    def pdf(x):
        return norm_const*np.exp(-0.5 * ((x-mean)/std)**2) * \
            np.sin(np.pi/180.0 * x)
    norm_dev = quad(pdf, 0.0, 180.0)[0]
    # ensure that the integral over the distribution equals 1
    norm_const /= norm_dev 
    return pdf


def _get_weights_asp(hydrom, hydrom_type, list_D, list_asp):
    '''
        Computes all probability distribution function (weights) against Dmax and aspect ratio
        Args:
            hydrom: a Hydrometeor Class instance (see hydrometeors.py)
            list_D: list of diameters in mm for which to compute the scattering properties.
            list_asp: list of aspect ratio (>1, Horizontal vs Rotational axis), 
            as level A database provided. But PDF are computed by inverse of aspect_ratio (>1, Horizontal vs Rotational axis), 
            assuming that the list_asp is equally spaced in 'inverse of aspect_ratio' domain.
            
        Returns:
            weights_asp: a xarray of weights against list_D and list_asp (>1, 
            Horizontal vs Rotational axis), for integration in _integration_over_pdf.
    '''
    
    params_dict = {
        'lambda_a': ASP_LAMBDA_A[hydrom_type],
        'lambda_b': ASP_LAMBDA_B[hydrom_type],
        'mu_a': ASP_MU_A[hydrom_type],
        'mu_b': ASP_MU_B[hydrom_type],        
    }
    params_shape = [len(value) for value in params_dict.values()]

    # formulate the numpy array
    wgt_shape = [len(list_D), len(list_asp)]
    wgt_shape.extend(params_shape)
    asp_wgt = np.empty(tuple(wgt_shape), dtype='float32')

    # formulate the xarray
    coords = [("Dmax", list_D), ("aspect_ratio", list_asp)]
    coords.extend(params_dict.items())
    weights_asp = xr.DataArray(asp_wgt, coords=coords) 

    # formulate the iteration
    iter_idx    = tuple([range(len(value)) for value in params_dict.values()])
    iter_keys   = params_dict.keys()
    key2ikey = dict()
    for ikey, key in enumerate(iter_keys):
        key2ikey[key] = ikey    
    
    # start iteration
    for idx_list in itertools.product(*iter_idx):
        # print(idx_list) # list
        lambda_a = params_dict['lambda_a'][idx_list[key2ikey['lambda_a']]]
        lambda_b = params_dict['lambda_b'][idx_list[key2ikey['lambda_b']]]
        mu_a = params_dict['mu_a'][idx_list[key2ikey['mu_a']]]
        mu_b = params_dict['mu_b'][idx_list[key2ikey['mu_b']]]

        ar_lambda = lambda_a * list_D**lambda_b
        ar_mu = mu_a * list_D**mu_b
        ar_loc = np.ones((len(list_D,)))

        # discard the invalid lambda_a and lambda_b pairs
        if ar_lambda[-1] < 1.0:
            loc_dict = dict(
                lambda_a=lambda_a, lambda_b=lambda_b,
                mu_a = mu_a, mu_b=mu_b,
            )
            weights_asp.loc[loc_dict] = np.nan
            continue

        # normal cases
        for l in zip(list_D, ar_lambda, ar_loc, ar_mu):
            # print(l)
            gamm = stats.gamma(l[1],l[2],l[3])
            wei = gamm.pdf(list_asp)
            wei /= np.sum(wei) # renormalization
            # print(wei)
            
            loc_dict = dict(
                lambda_a=lambda_a, lambda_b=lambda_b,
                mu_a = mu_a, mu_b=mu_b, Dmax=l[0]
            )
            weights_asp.loc[loc_dict] = wei

        # exit()
    
    # (Dmax, aspect_ratio, lambda_a, lambda_b, mu_a, mu_b)
    # print(weights_asp)
    # exit()

    return weights_asp

def _get_weights_beta(hydrom, hydrom_type, list_D, list_beta):
    '''
        Computes all probability distribution function (weights) against Dmax and Euler angle beta
        Args:
            hydrom: a Hydrometeor Class instance (see hydrometeors.py)
            list_D: list of diameters in mm for which to compute the scattering
                properties
            list_beta: list of beta, in range of [0, pi/2.], as level A database provided 
            
        Returns:
            weights_beta: a xarray of weights against list_D and list_beta [0, pi]
            for integration in _integration_over_pdf
    '''
    list_beta_padded = np.pad(list_beta, (0,len(list_beta)-1), 'reflect', reflect_type='odd')

    params_dict = {
        'std_a': CANT_STD_A[hydrom_type],
        'std_b': CANT_STD_B[hydrom_type],       
    }
    params_shape = [len(value) for value in params_dict.values()]

    # formulate the numpy array
    wgt_shape = [len(list_D), len(list_beta_padded)]
    wgt_shape.extend(params_shape)
    beta_wgt = np.empty(tuple(wgt_shape), dtype='float32')

    # formulate the xarray
    coords = [("Dmax", list_D), ("beta", list_beta_padded)]
    coords.extend(params_dict.items())
    weights_beta = xr.DataArray(beta_wgt, coords=coords) 

    # formulate the iteration
    iter_idx    = tuple([range(len(value)) for value in params_dict.values()])
    iter_keys   = params_dict.keys()
    key2ikey = dict()
    for ikey, key in enumerate(iter_keys):
        key2ikey[key] = ikey

    # start iteration
    for idx_list in itertools.product(*iter_idx):
        # print(idx_list)
        std_a = params_dict['std_a'][idx_list[key2ikey['std_a']]]
        std_b = params_dict['std_b'][idx_list[key2ikey['std_b']]]

        beta_std = std_a * list_D**std_b

        for iD, D in enumerate(list_D):
            guassian = gaussian_pdf(std=beta_std[iD])
            wei = guassian(list_beta_padded)
            wei /= np.sum(wei) # renormalization
            
            loc_dict = dict(
                std_a=std_a, std_b=std_b,
                Dmax=D
            )
            weights_beta.loc[loc_dict] = wei

    # print(weights_beta)
    # exit()
    
    return weights_beta

def _integrate_over_pdf(hydrom, hydrom_type, frequency, elevation, temperature, list_D, list_beta, list_asp, levelb_name_folder):
    '''
        Computes all scattering properties for a given set of parameters and
        for a given hydrometeor, according its probability distribution function of
        orientation (orientation Euler angle beta and aspect ratio).
        This is to be used for all hydrometeors except melting snow and
        melting graupel (currently only snow implemented)
        Args:
            hydrom: a Hydrometeor Class instance (see hydrometeors.py)
            freqency: the frequency in GHz
            elevation: incident elevation angle in degrees
            temperature: the temperature in K
            list_D: list of diameters in mm for which to compute the scattering
                properties
            list_beta: list of Euler angle beta ([0, pi/2.]), as level A database providing
            list_asp: list of aspect ratio (<1.), as level A database providing
            
        Returns:
            NO RETURNS, but save the piece of database in your disk
    '''
    
    ds_in = levela_lut.sel(temperature=temperature, elevation=elevation)

    # get weights xarray
    if ARS[hydrom_type] != 'SINGLE':
        weights_asp = _get_weights_asp(hydrom, hydrom_type, list_D, list_asp)
    weights_beta = _get_weights_beta(hydrom, hydrom_type, list_D, list_beta)

    # start formulating the ds_out
    coords = {'Dmax': list_D,
    'lambda_a': ASP_LAMBDA_A[hydrom_type], 'lambda_b': ASP_LAMBDA_B[hydrom_type],
    'mu_a': ASP_MU_A[hydrom_type], 'mu_b': ASP_MU_B[hydrom_type],
    'std_a': CANT_STD_A[hydrom_type], 'std_b': CANT_STD_B[hydrom_type]}
    datadic = {}
    for var in ds_in.data_vars.keys():
        # Padding the DataArray in beta axis
        padded_da = ds_in[var].pad(beta=(0,len(list_beta)-1), mode='reflect')
        padded_da.coords['beta'] = np.pad(list_beta, (0, len(list_beta)-1), 'reflect', reflect_type='odd')

        # perform intergration over canting angle and aspect ratio probability distribution
        weights = weights_beta.fillna(0.0)
        mean = padded_da.weighted(weights).sum('beta')
        datadic[var] = mean.where(mean>0, np.nan)
        if ARS[hydrom_type] != 'SINGLE':
            weights = weights_asp.fillna(0.0)
            mean = datadic[var].weighted(weights).sum('aspect_ratio')
            datadic[var] = mean.where(mean>0, np.nan)

    ds_out = xr.Dataset(datadic, coords=coords)

    # print(ds_out['p11_bw'])
    # print(ds_out)
    # exit()

    print('temperature={}, elevation={}'.format(temperature, elevation))

    levelb_name_lut = levelb_name_folder + 't{}e{}.nc'.format(temperature, elevation)
    ds_out.to_netcdf(levelb_name_lut, engine="h5netcdf")
    del ds_out

    return
    
def sz_lut(hydrom_type, frequency, levela_name_lut, levelb_name_folder):
    """
        Computes and saves a scattering lookup table (level B database) for a given
        hydrometeor type (non melting) and various frequencies from level A database.
        Args:
            hydrom_type: the hydrometeor type, currently only snow is implemented.
            frequency: list of frequencies for which to obtain the
                lookup tables, in GHz.
            levela_name_lut: the file name of level A database (Input).
            levelb_name_folder: the folder name of level B database (Output).
        Returns:
            No output but saves a lookup table (levelb_name_lut).
    """

    scheme = '1mom'
    
    hydrom = create_hydrometeor(hydrom_type,'1mom')	
    
    global levela_lut

    with xr.open_dataset(levela_name_lut, engine="h5netcdf") as levela_lut:
        # Dimensions that formulate the SZ_matices
        list_elevation = levela_lut.coords["elevation"].values
        list_temperature = levela_lut.coords['temperature'].values
        list_D = levela_lut.coords["Dmax"].values
        # Note: the list_D should meet the hydrometeor list_D for fitted IITM database

        # Dimensions that should be integrated over probability distribution 
        # in levelA to levelB transformation
        if ARS[hydrom_type] != 'SINGLE':
            list_asp = levela_lut.coords['aspect_ratio'].values
        else:
            list_asp = None
        list_beta = levela_lut.coords['beta'].values

        '''
        Place to hold the integration over probability distribution
        '''
        pool = mp.Pool(processes=mp.cpu_count())
        for e in list_elevation:
            for t in list_temperature:
                # _integrate_over_pdf(hydrom, hydrom_type, frequency, e, t, list_D, list_beta, list_asp)
                # exit()
                args = (hydrom, hydrom_type, frequency, e, t, list_D, list_beta, list_asp, levelb_name_folder)
                pool.apply_async(_integrate_over_pdf, args=args)

        # Gather processes of multiprocess
        pool.close()
        pool.join()

if __name__ == "__main__":
    '''
       Create all lookup tables for the specified hydrometeor types and
        microphysical schemes (currently only 1mom scheme implemented...)
    '''

    scheme = '1mom'
    
    for frequency in FREQUENCIES:
        for hydrom_type in HYDROM_TYPES:
            
            # The name of the lookup table is lut_SZ_<hydro_name>_<freq>_<scheme>_<level_name>.nc

            levela_name_lut = (FOLDER_LUT+"lut_SZ_"+hydrom_type+'_'+
                    str(frequency).replace('.','_')+'_'+scheme+"_LevelA"+".nc")

            levelb_name_folder = (FOLDER_LUT+"lut_SZ_"+hydrom_type+'_'+
                    str(frequency).replace('.','_')+'_'+scheme+"_LevelB_exp/")

            if not os.path.exists(levelb_name_folder):
                os.system("mkdir {}".format(levelb_name_folder))
            
            if (FORCE_REGENERATION_SCATTER_TABLES
                or not os.path.exists(levelb_name_lut)):
                msg = '''
                Generating scatter table for 1 moment scheme,
                hydrometeor = {:s}
                freq = {:s}
                '''.format(hydrom_type, str(frequency))
                print(msg)

                sz_lut(hydrom_type, frequency, levela_name_lut, levelb_name_folder)
